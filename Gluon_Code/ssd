from mxnet import gluon
from mxnet import nd
from mxnet import image
from mxnet import autograd
from mxnet.contrib.ndarray import MultiBoxPrior
from mxnet.contrib.ndarray import MultiBoxTarget
from mxnet.gluon import nn
from mxnet import metric
import matplotlib as mpl
import time
mpl.rcParams['figure.dpi'] = 120
import matplotlib.pyplot as plt
from NetLoss.focal_loss import FocalLoss
from NetLoss.smooth_l1 import SmoothL1Loss


root_url = ('https://apache-mxnet.s3-accelerate.amazonaws.com/'
            'gluon/dataset/pikachu/')
data_dir = '../data/pikachu/'

dataset = {'train.rec': 'e6bcb6ffba1ac04ff8a9b1115e650af56ee969c8',
           'train.idx': 'dcf7318b2602c06428b9988470c731621716c393',
           'val.rec': 'd6c33f799b4d058e82f2cb5bd9a976f69d72d520'}

colors = ['blue', 'green', 'red', 'black', 'magenta']


def download_pikachu():
    for k, v in dataset.items():
        gluon.utils.download(root_url + k, data_dir + k, sha1_hash=v)


def get_iterator(data_shape, batch_size):
    class_names = ['pikachu']
    num_class = len(class_names)
    train_iter = image.ImageDetIter(
        batch_size=batch_size,
        data_shape=(3, data_shape, data_shape),
        path_imgrec=data_dir + 'train.rec',
        path_imgidx=data_dir + 'train.idx',
        shuffle=True,
        mean=True,
        rand_crop=1,
        min_object_covered=0.95,
        max_attempts=200
    )
    val_iter = image.ImageDetIter(
        batch_size=batch_size,
        data_shape=(3, data_shape, data_shape),
        path_imgrec=data_dir + 'val.rec',
        shuffle=False,
        mean=True
    )
    return train_iter, val_iter, class_names, num_class

# batch_size = 32
# data_shape = 256
# rgb_mean = nd.array([123, 117, 104])
# train_data, test_data, class_names, num_class = get_iterator(data_shape, batch_size)

# batch = train_data.next()
# print(batch)

def box_to_rect(box, color, linewidth=3):
    box = box.asnumpy()
    return plt.Rectangle(
        (box[0], box[1]), box[2] - box[0],  box[3] - box[1],
        fill=False, edgecolor=color, linewidth=linewidth
    )

def show_pikachu():
    _, figs = plt.subplots(3, 3, figsize=(6,6))

    for i in range(3):
        for j in range(3):
            img, labels = batch.data[0][3 * i + j], batch.label[0][3 * i + j]
            img = img.transpose((1, 2, 0)) + rgb_mean
            img = img.clip(0, 255).asnumpy()/ 255
            fig = figs[i][j]
            fig.imshow(img)
            for label in labels:
                rect = box_to_rect(label[1:5] * data_shape, 'red', 2)
                fig.add_patch(rect)
            fig.axes.get_xaxis().set_visible(False)
            fig.axes.get_yaxis().set_visible(False)

    plt.show()


def show_box():
    n = 40

    x = nd.random.normal(shape=(1, 3, n, n))

    y = MultiBoxPrior(x, sizes=[.5, .25, .1], ratios=[1, 2, .5])
    print('shape of y ', y.shape)
    boxes = y.reshape((n, n, -1, 4))

    print(boxes.shape)

    print(boxes[20, 20, 0, :])

    plt.imshow(nd.ones((n, n, 3)).asnumpy())

    anchors = boxes[0, 0, :, :]

    for i in range(anchors.shape[0]):
        plt.gca().add_patch(box_to_rect(anchors[i, :] * n, colors[i]))

    plt.show()


def predict_box(num_anchors):
    return nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1, strides=1)


def class_predictor(num_anchors, num_classes):
    return nn.Conv2D(num_anchors * (num_classes + 1), kernel_size=3, padding=1, strides=1)


def down_sampler(num_filters):
    net = nn.HybridSequential()
    with net.name_scope():
        for _ in range(2):
            net.add(
                nn.Conv2D(num_filters, 3, padding=1, strides=1),
                nn.BatchNorm(in_channels=num_filters),
                nn.Activation('relu')
            )
        net.add(nn.MaxPool2D(pool_size=2))
        return net


def body(num_filters):
    net = nn.HybridSequential()
    with net.name_scope():
        for filters in num_filters:
            net.add(down_sampler(filters))
    return net


def flatten_prediction(pred):
    return pred.transpose(axes=(0, 2, 3, 1)).flatten()


def concat_predictions(preds):
    return nd.concat(*preds)


def toy_ssd_model(num_anchors, num_classes):
    down_samplers = nn.Sequential()
    for _ in range(3):
        down_samplers.add(down_sampler(128))
    class_predictors = nn.Sequential()
    box_predictors = nn.Sequential()
    for _ in range(5):
        class_predictors.add(class_predictor(num_anchors, num_classes))
        box_predictors.add(predict_box(num_anchors))
    model = nn.Sequential()
    model.add(body([16, 32, 64]), down_samplers, class_predictors, box_predictors)
    return model


def toy_ssd_forward(x, model, sizes, ratios, verbose=False):
    body_net, down_samplers, class_predictors, box_predictors = model
    x = body_net(x)
    anchors, class_preds, box_preds = [], [], []
    for i in range(5):
        anchor = MultiBoxPrior(x, sizes[i], ratios[i])
        anchors.append(anchor)
        class_preds.append(
            flatten_prediction(class_predictors[i](x))
        )
        box_preds.append(
            flatten_prediction(box_predictors[i](x))
        )
        if verbose:
            print('Predict scale', i, x.shape, 'with', anchors[-1].shape[1], 'anchors')
        if i < 3:
            x = down_samplers[i](x)
        elif i == 3:
            x = nd.Pooling(x, global_pool=True, pool_type='max',
                           kernel=(x.shape[2], x.shape[3]))
    return (concat_predictions(anchors),
            concat_predictions(class_preds),
            concat_predictions(box_preds))


x = nd.random.normal(shape=(2, 8, 20, 20))

class_pred1 = class_predictor(5, 10)
class_pred1.initialize()
y1 = class_pred1(x)
print('shape of y1:', y1.shape)
ds = down_sampler(16)
ds.initialize()
x = ds(x)

class_pred2 = class_predictor(3, 10)
class_pred2.initialize()
y2 = class_pred2(x)
print('shape of y2:', y2.shape)

# show_box()

y1_flatten = flatten_prediction(y1)
y2_flatten = flatten_prediction(y2)

concat_result = concat_predictions([y1_flatten, y2_flatten])
print('shape of y1 flatten', y1_flatten.shape, ' shape of y2 flatten', y2_flatten.shape)
print('shape of concat ', concat_result.shape)

class ssd(nn.Sequential):
    def __init__(self, num_classes, verbose=False, **kwargs):
        super(ssd, self).__init__(**kwargs)
        self.sizes = [[.2, .272], [.37, .447], [.54, .619],
                      [.71, .79], [.88, .961]]
        self.ratios = [[1, 2, .5]] * 5
        self.num_classes = num_classes
        self.anchors = len(self.sizes[0]) + len(self.ratios[0]) - 1
        self.verbose = verbose
        with self.name_scope():
            self.model = toy_ssd_model(self.anchors, self.num_classes)
        print('done')


    def forward(self, x):
        anchors, class_preds, box_preds = toy_ssd_forward(x, self.model, self.sizes, self.ratios,
                                                          self.verbose)
        class_preds = class_preds.reshape(0, -1, self.num_classes + 1)
        return anchors, class_preds, box_preds


def training_target(anchors, class_preds, labels):
    class_preds = class_preds.transpose(axes=(0, 2, 1))
    return MultiBoxTarget(anchors, labels, class_preds)



# net = ssd(num_classes=2, verbose=True)
# net.initialize()

# x = nd.random.normal(shape=(1, 3, 256, 256))
# y = net(x)

data_shape = 256
batch_size = 32
train_data, test_data, class_names, num_classes = get_iterator(data_shape, batch_size)
net = ssd(num_classes=num_classes)
net.initialize()
epochs = 10
cls_loss = FocalLoss()
box_loss = SmoothL1Loss()
cls_metric = metric.Accuracy()
box_metric = metric.MAE()

trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'wd': 5e-4})
start = time.time()
for epoch in range(epochs):
    train_data.reset()
    cls_metric.reset()
    box_metric.reset()
    start = time.time()
    for batch in train_data:
        data = batch.data[0]
        label = batch.label[0]
        with autograd.record():
            anchors, class_preds, box_preds = net(data)
            box_target, box_mask, cls_target = training_target(anchors, class_preds, label)
            loss1 = cls_loss(class_preds, cls_target)
            loss2 = box_loss(box_preds, box_target, box_mask)
            loss = loss1 + loss2
        loss.backward()
        trainer.step(batch_size=batch_size)
        cls_metric.update([cls_target], [class_preds.transpose(0, 2, 1)])
        box_metric.update([box_target], [box_preds * box_mask])

    print('Epoch %2d, train %s %.2f, %s %.2f, time %.1f sec' % (
        epoch, *cls_metric.get(), *box_metric.get(), time.time() - start
    ))

